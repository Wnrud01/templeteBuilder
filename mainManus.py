# import os
# import chromadb
# from typing import TypedDict, List

# # Pydantic ë° LangChain í˜¸í™˜ì„±ì„ ìœ„í•œ ì„í¬íŠ¸
# from pydantic import PrivateAttr

# # LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_community.document_loaders import TextLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import Chroma
# from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
# from langchain_community.retrievers import BM25Retriever
# from langchain_core.documents import Document
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langgraph.graph import StateGraph, END
# from dotenv import load_dotenv

# # FlashRank ì„í¬íŠ¸
# try:
#     from flashrank import Ranker, RerankRequest
#     from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
#     from langchain_core.callbacks.manager import Callbacks
# except ImportError:
#     print("FlashRank ë˜ëŠ” ê´€ë ¨ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install flashrank'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
#     BaseDocumentCompressor = object # ì„ì‹œ ì •ì˜
#     Ranker = None

# # ValidationError í•´ê²°ì„ ìœ„í•´ PrivateAttr ì‚¬ìš©
# class FlashRankRerank(BaseDocumentCompressor):
#     """FlashRankë¥¼ ì‚¬ìš©í•œ LangChain ë¬¸ì„œ ì¬ì •ë ¬ê¸°"""
    
#     _ranker: Ranker = PrivateAttr()
#     top_n: int = 3

#     def __init__(self, *args, **kwargs):
#         super().__init__(*args, **kwargs)
#         self._ranker = Ranker()

#     class Config:
#         arbitrary_types_allowed = True

#     def compress_documents(
#         self,
#         documents: List[Document],
#         query: str,
#         callbacks: Callbacks | None = None,
#     ) -> List[Document]:
#         if not documents:
#             return []
        
#         rerank_request = RerankRequest(
#             query=query,
#             passages=[{"id": i, "text": doc.page_content} for i, doc in enumerate(documents)]
#         )
#         reranked_results = self._ranker.rerank(rerank_request)
        
#         final_docs = []
#         for item in reranked_results[:self.top_n]:
#             original_doc_index = item['id']
#             doc = documents[original_doc_index]
#             doc.metadata["relevance_score"] = item['score']
#             final_docs.append(doc)
#         return final_docs

# # --- 0. ì´ˆê¸° ì„¤ì •: API í‚¤ ë¡œë”© ---
# load_dotenv()
# if not os.getenv("OPENAI_API_KEY"):
#     raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

# # --- 1. ë°ì´í„° ì¤€ë¹„: ì™¸ë¶€ íŒŒì¼ì—ì„œ í…œí”Œë¦¿ ë¡œë”© ---
# # --- [ìˆ˜ì •ëœ ë¶€ë¶„] íŒŒì¼ í˜•ì‹ì— ë§ëŠ” ë‘ ê°€ì§€ ë¡œë“œ í•¨ìˆ˜ ì •ì˜ ---
# def load_line_by_line(file_path: str) -> List[str]:
#     """í•œ ì¤„ì— í•œ í•­ëª©ì”© ìˆëŠ” íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     try:
#         with open(file_path, 'r', encoding='utf-8') as f:
#             items = [line.strip() for line in f if line.strip()]
#         print(f"âœ… {len(items)}ê°œì˜ í•­ëª©ì„ '{file_path}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
#         return items
#     except FileNotFoundError:
#         print(f"ğŸš¨ ê²½ê³ : '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
#         return []

# def load_by_separator(file_path: str, separator: str = '---') -> List[str]:
#     """êµ¬ë¶„ìë¡œ ë¶„ë¦¬ëœ í•­ëª©ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
#     try:
#         with open(file_path, 'r', encoding='utf-8') as f:
#             content = f.read()
#             items = [section.strip() for section in content.split(separator) if section.strip()]
#         print(f"âœ… {len(items)}ê°œì˜ í•­ëª©ì„ '{file_path}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
#         return items
#     except FileNotFoundError:
#         print(f"ğŸš¨ ê²½ê³ : '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
#         return []

# # ê° íŒŒì¼ì— ë§ëŠ” ë¡œë“œ í•¨ìˆ˜ í˜¸ì¶œ
# APPROVED_TEMPLATES = load_line_by_line("./data/approved_templates.txt")
# REJECTED_TEMPLATES_TEXT = load_by_separator("./data/rejected_templates.txt")

# # ì „ì—­ ë³€ìˆ˜ë¡œ Retrieverë¥¼ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ë¡œë”© ë°©ì§€
# retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected = None, None, None, None

# def setup_retrievers():
#     global retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected
#     if all([retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected]):
#         print("Retrieverê°€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
#         return

#     print("Retriever ì„¤ì • ì‹œì‘...")
    
#     from chromadb.config import Settings

#     docs_compliance = TextLoader("./data/compliance_rules.txt", encoding='utf-8').load()
#     docs_generation = TextLoader("./data/generation_rules.txt", encoding='utf-8').load()
    
#     docs_whitelist = [Document(page_content=t) for t in APPROVED_TEMPLATES]
#     docs_rejected = [Document(page_content=t) for t in REJECTED_TEMPLATES_TEXT]

#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
#     split_compliance = text_splitter.split_documents(docs_compliance)
#     split_generation = text_splitter.split_documents(docs_generation)
#     # ë°˜ë ¤ í…œí”Œë¦¿ì€ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡ ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#     split_rejected = docs_rejected

#     embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
#     vector_db_path = "./vector_db"
    
#     client_settings = Settings(anonymized_telemetry=False)
    
#     db_compliance = Chroma.from_documents(split_compliance, embeddings, collection_name="compliance_rules", persist_directory=vector_db_path, client_settings=client_settings)
#     db_generation = Chroma.from_documents(split_generation, embeddings, collection_name="generation_rules", persist_directory=vector_db_path, client_settings=client_settings)
#     db_whitelist = Chroma.from_documents(docs_whitelist, embeddings, collection_name="whitelist_templates", persist_directory=vector_db_path, client_settings=client_settings)
#     db_rejected = Chroma.from_documents(split_rejected, embeddings, collection_name="rejected_templates", persist_directory=vector_db_path, client_settings=client_settings)

#     def create_hybrid_retriever(vectorstore, docs):
#         if not docs:
#             return vectorstore.as_retriever(search_kwargs={"k": 5})

#         vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
#         keyword_retriever = BM25Retriever.from_documents(docs)
#         keyword_retriever.k = 5

#         ensemble_retriever = EnsembleRetriever(retrievers=[vector_retriever, keyword_retriever], weights=[0.5, 0.5])
        
#         if Ranker:
#             compressor = FlashRankRerank(top_n=3)
#             return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble_retriever)
#         return ensemble_retriever

#     retriever_compliance = create_hybrid_retriever(db_compliance, split_compliance)
#     retriever_generation = create_hybrid_retriever(db_generation, split_generation)
#     retriever_whitelist = create_hybrid_retriever(db_whitelist, docs_whitelist)
#     retriever_rejected = create_hybrid_retriever(db_rejected, split_rejected)
    
#     print("âœ… Retriever ì„¤ì • ì™„ë£Œ!")

# # --- 2. LangGraph íŒŒì´í”„ë¼ì¸ ì •ì˜ ---
# class GraphState(TypedDict):
#     original_request: str
#     hypothetical_template: str
#     template_draft: str
#     retrieved_examples: str
#     retrieved_rules: str
#     retrieved_rejections: str
#     validation_result: str
#     correction_attempts: int

# llm = ChatOpenAI(model="gpt-4o", temperature=0.4)

# hyde_prompt = ChatPromptTemplate.from_template(
#     """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê³  ê·¸ì— ë§ëŠ” ì´ìƒì ì¸ ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ìƒìƒí•´ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
#     ì˜¤ì§ ìš”ì²­ì— ë¶€í•©í•˜ëŠ” ì™„ë²½í•œ í…œí”Œë¦¿ í…ìŠ¤íŠ¸ë§Œ ìƒì„±í•˜ê³ , ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.

#     ì‚¬ìš©ì ìš”ì²­: "{request}"

#     ê°€ìƒ í…œí”Œë¦¿:
#     """
# )

# generation_prompt = ChatPromptTemplate.from_template(
#     """ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹¤ìˆ˜ë¥¼ í†µí•´ ë°°ìš°ëŠ” ì•Œë¦¼í†¡ í…œí”Œë¦¿ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

#     # ì§€ì‹œì‚¬í•­
#     1. 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì •ë³´ì„± ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ì‘ì„±í•˜ì„¸ìš”.
#     2. **(ì¤‘ìš”) 'ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€'ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì—¬ ë™ì¼í•œ ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.**
#     3. 'ì°¸ê³ ìš© ì„±ê³µ ì‚¬ë¡€'ì˜ ìŠ¤íƒ€ì¼ì„ ë”°ë¥´ë˜, ì ˆëŒ€ ë˜‘ê°™ì´ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
#     4. 'í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™'ì„ ì§€í‚¤ê³ , #{{ë³€ìˆ˜}} ì‚¬ìš©ë²•ì„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
#     5. ë§Œì•½ ì‚¬ìš©ì ìš”ì²­ì´ ê´‘ê³ ì„±ì´ë¼ë©´, ì •ë³´ì„±ìœ¼ë¡œ ë°œì†¡ ë¶ˆê°€í•˜ë‹¤ê³  ì•ˆë‚´í•˜ê³  'ì¹œêµ¬í†¡' ì‚¬ìš©ì„ ê¶Œì¥í•˜ì„¸ìš”.

#     # ì‚¬ìš©ì ì›ë³¸ ìš”ì²­
#     {original_request}

#     # ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€ (ì˜¤ë‹µ ë…¸íŠ¸)
#     {rejections}

#     # ì°¸ê³ ìš© ì„±ê³µ ì‚¬ë¡€
#     {examples}

#     # í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™
#     {rules}

#     # ì „ë¬¸ê°€ ë‹µë³€ (ìœ„ ì§€ì‹œì‚¬í•­ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ì‘ì„±):
#     """
# )

# step_back_prompt = ChatPromptTemplate.from_template(
#     """ë‹¹ì‹ ì€ ë¬¸ì œì˜ ë³¸ì§ˆì„ íŒŒì•…í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.
#     ì£¼ì–´ì§„ 'í…œí”Œë¦¿ ì´ˆì•ˆ'ì„ ë³´ê³ , ì´ í…œí”Œë¦¿ì´ ì •ë³´í†µì‹ ë§ë²•ì´ë‚˜ ë‚´ë¶€ ê·œì •ê³¼ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ í•µì‹¬ì ì¸ ì§ˆë¬¸ì„ ë˜ì§€ëŠ”ì§€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

#     # í…œí”Œë¦¿ ì´ˆì•ˆ
#     {draft}

#     # í•µì‹¬ ì§ˆë¬¸:
#     """
# )

# validation_prompt = ChatPromptTemplate.from_template(
#     """ë‹¹ì‹ ì€ ê³¼ê±° íŒë¡€ê¹Œì§€ ì°¸ê³ í•˜ëŠ” ë§¤ìš° ê¼¼ê¼¼í•œ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
    
#     # ê²€ìˆ˜ ëŒ€ìƒ í…œí”Œë¦¿
#     {draft}

#     # ê´€ë ¨ ê·œì •
#     {rules}

#     # ìœ ì‚¬í•œ ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€ (íŒë¡€)
#     {rejections}

#     # ì§€ì‹œì‚¬í•­
#     1. 'ê²€ìˆ˜ ëŒ€ìƒ í…œí”Œë¦¿'ì´ 'ìœ ì‚¬í•œ ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€'ì™€ ë¹„ìŠ·í•œ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.
#     2. ê·¸ ë‹¤ìŒ 'ê´€ë ¨ ê·œì •'ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ì—¬ ìµœì¢… ìœ„ë°˜ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
#     3. ìœ„ë°˜ ì‚¬í•­ì´ ì—†ë‹¤ë©´ "accept" ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
#     4. ìœ„ë°˜ ì‚¬í•­ì´ ìˆë‹¤ë©´, ì–´ë–¤ ê·œì • ë˜ëŠ” ê³¼ê±° ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ìœ„ë°˜ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
#     """
# )

# def hyde_generation(state: GraphState):
#     print("\n--- 1. HyDE ê°€ìƒ í…œí”Œë¦¿ ìƒì„± ì‹œì‘ ---")
#     request = state['original_request']
#     chain = hyde_prompt | llm | StrOutputParser()
#     hypothetical_template = chain.invoke({"request": request})
#     state['hypothetical_template'] = hypothetical_template
#     print("âœ… HyDE ìƒì„± ì™„ë£Œ.")
#     return state

# def generate_draft(state: GraphState):
#     print("\n--- 2. í…œí”Œë¦¿ ì´ˆì•ˆ ìƒì„± ì‹œì‘ ---")
#     original_request = state['original_request']
#     query = state['hypothetical_template']
    
#     example_docs = retriever_whitelist.invoke(query)
#     state['retrieved_examples'] = "\n\n".join([f"ì˜ˆì‹œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(example_docs)])
    
#     rule_docs = retriever_generation.invoke(query)
#     state['retrieved_rules'] = "\n\n".join([doc.page_content for doc in rule_docs])
    
#     rejected_docs = retriever_rejected.invoke(query)
#     state['retrieved_rejections'] = "\n\n".join([doc.page_content for doc in rejected_docs])
    
#     chain = generation_prompt | llm | StrOutputParser()
#     draft = chain.invoke({
#         "original_request": original_request, 
#         "rejections": state['retrieved_rejections'],
#         "examples": state['retrieved_examples'], 
#         "rules": state['retrieved_rules']
#     })
#     state['template_draft'] = draft
#     state['correction_attempts'] = 0
#     print("âœ… ì´ˆì•ˆ ìƒì„± ì™„ë£Œ.")
#     print(f"ìƒì„±ëœ ì´ˆì•ˆ:\n---\n{draft}\n---")
#     return state

# def validate_draft(state: GraphState):
#     print("\n--- 3. ê·œì • ì¤€ìˆ˜ ê²€ì¦ ì‹œì‘ ---")
#     draft = state['template_draft']
    
#     if "ê´‘ê³ ì„±" in draft or "ì¹œêµ¬í†¡" in draft:
#         state['validation_result'] = "accept"
#         print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼ (ìƒì„± ë‹¨ê³„ì—ì„œ ê´‘ê³ ì„±ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ì•ˆë‚´í•¨)")
#         return state

#     print("   - 3a. Step-Back í•µì‹¬ ìŸì  ìƒì„± ì¤‘...")
#     step_back_chain = step_back_prompt | llm | StrOutputParser()
#     step_back_question = step_back_chain.invoke({"draft": draft})
#     print(f"   - ìƒì„±ëœ í•µì‹¬ ìŸì : {step_back_question}")

#     print("   - 3b. ê´€ë ¨ ê·œì • ë° ë°˜ë ¤ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
#     compliance_docs = retriever_compliance.invoke(step_back_question)
#     rules = "\n\n".join([doc.page_content for doc in compliance_docs])
    
#     rejected_docs = retriever_rejected.invoke(draft)
#     rejections = "\n\n".join([doc.page_content for doc in rejected_docs])
    
#     print("   - 3c. ìµœì¢… ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
#     chain = validation_prompt | llm | StrOutputParser()
#     result = chain.invoke({"draft": draft, "rules": rules, "rejections": rejections})
#     state['validation_result'] = result
    
#     if "accept" in result.lower():
#         print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
#     else:
#         print(f"ğŸš¨ ê²€ì¦ ê²°ê³¼: ìœ„ë°˜ ë°œê²¬ (ì‹œë„ {state['correction_attempts'] + 1}íšŒ)")
#         print(f"ìœ„ë°˜ ë‚´ìš©: {result}")
#     return state

# def self_correct(state: GraphState):
#     print("\n--- 4. ìê°€ ìˆ˜ì • ì‹œì‘ ---")
    
#     correction_request = (
#         f"ì´ì „ í…œí”Œë¦¿ì€ ë‹¤ìŒì˜ ì´ìœ ë¡œ ë°˜ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤: '{state['validation_result']}'. "
#         f"ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì›ë˜ ìš”ì²­ì¸ '{state['original_request']}'ì„ ë§Œì¡±í•˜ëŠ” ìƒˆë¡œìš´ ì •ë³´ì„± í…œí”Œë¦¿ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
#     )
    
#     chain = generation_prompt | llm | StrOutputParser()
#     new_draft = chain.invoke({
#         "original_request": correction_request,
#         "rejections": state['retrieved_rejections'],
#         "examples": state['retrieved_examples'],
#         "rules": state['retrieved_rules']
#     })
    
#     state['template_draft'] = new_draft
#     state['correction_attempts'] += 1
#     print("âœ… ìê°€ ìˆ˜ì • ì™„ë£Œ.")
#     return state

# def decide_next_step(state: GraphState):
#     if "accept" in state['validation_result'].lower():
#         return "end"
#     elif state['correction_attempts'] >= 1:
#         return "end_with_failure"
#     else:
#         return "self_correct"

# def build_graph():
#     workflow = StateGraph(GraphState)
    
#     workflow.add_node("hyde_generation", hyde_generation)
#     workflow.add_node("generate_draft", generate_draft)
#     workflow.add_node("validate_draft", validate_draft)
#     workflow.add_node("self_correct", self_correct)
    
#     workflow.set_entry_point("hyde_generation")
    
#     workflow.add_edge("hyde_generation", "generate_draft")
#     workflow.add_edge("generate_draft", "validate_draft")
#     workflow.add_conditional_edges(
#         "validate_draft",
#         decide_next_step,
#         {"self_correct": "self_correct", "end": END, "end_with_failure": END}
#     )
#     workflow.add_edge("self_correct", "validate_draft")
    
#     return workflow.compile()

# # --- 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
# if __name__ == "__main__":
#     setup_retrievers()
#     app = build_graph()

#     user_request = "ì•ˆë…•í•˜ì„¸ìš” #{íšŒì›ëª…}ë‹˜, ê³ ìš©ë…¸ë™ë¶€ í”Œë«í¼ ì´ìš© ì§€ì›ì— ëŒ€í•´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤."
    
#     print(f"\n==================================================")
#     print(f"ì‚¬ìš©ì ìš”ì²­: {user_request}")
#     print(f"==================================================")
    
#     final_state = app.invoke({"original_request": user_request})

#     print(f"\n================ ìµœì¢… ê²°ê³¼ ================")
#     if final_state and "accept" in final_state.get('validation_result', '').lower():
#         print("ğŸ‰ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
#         print(final_state['template_draft'])
#     elif final_state:
#         print("ğŸ”¥ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì‹œë„ ê²°ê³¼:")
#         print(f"ì‹¤íŒ¨ ì‚¬ìœ : {final_state.get('validation_result', 'N/A')}")
#         print(f"ë§ˆì§€ë§‰ ì´ˆì•ˆ:\n{final_state.get('template_draft', 'N/A')}")
#     else:
#         print("ì˜¤ë¥˜: íŒŒì´í”„ë¼ì¸ì´ ìµœì¢… ìƒíƒœë¥¼ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#     print("============================================")

import os
import chromadb
from typing import TypedDict, List

# Pydantic ë° LangChain í˜¸í™˜ì„±ì„ ìœ„í•œ ì„í¬íŠ¸
from pydantic import PrivateAttr

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

# FlashRank ì„í¬íŠ¸
try:
    from flashrank import Ranker, RerankRequest
    from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
    from langchain_core.callbacks.manager import Callbacks
except ImportError:
    print("FlashRank ë˜ëŠ” ê´€ë ¨ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install flashrank'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    BaseDocumentCompressor = object # ì„ì‹œ ì •ì˜
    Ranker = None

# ValidationError í•´ê²°ì„ ìœ„í•´ PrivateAttr ì‚¬ìš©
class FlashRankRerank(BaseDocumentCompressor):
    """FlashRankë¥¼ ì‚¬ìš©í•œ LangChain ë¬¸ì„œ ì¬ì •ë ¬ê¸°"""
    
    _ranker: Ranker = PrivateAttr()
    top_n: int = 3

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._ranker = Ranker()

    class Config:
        arbitrary_types_allowed = True

    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -> List[Document]:
        if not documents:
            return []
        
        rerank_request = RerankRequest(
            query=query,
            passages=[{"id": i, "text": doc.page_content} for i, doc in enumerate(documents)]
        )
        reranked_results = self._ranker.rerank(rerank_request)
        
        final_docs = []
        for item in reranked_results[:self.top_n]:
            original_doc_index = item['id']
            doc = documents[original_doc_index]
            doc.metadata["relevance_score"] = item['score']
            final_docs.append(doc)
        return final_docs

# --- 0. ì´ˆê¸° ì„¤ì •: API í‚¤ ë¡œë”© ---
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

# --- 1. ë°ì´í„° ì¤€ë¹„: ì™¸ë¶€ íŒŒì¼ì—ì„œ í…œí”Œë¦¿ ë¡œë”© ---
def load_line_by_line(file_path: str) -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            items = [line.strip() for line in f if line.strip()]
        print(f"âœ… {len(items)}ê°œì˜ í•­ëª©ì„ '{file_path}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return items
    except FileNotFoundError:
        print(f"ğŸš¨ ê²½ê³ : '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return []

def load_by_separator(file_path: str, separator: str = '---') -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            items = [section.strip() for section in content.split(separator) if section.strip()]
        print(f"âœ… {len(items)}ê°œì˜ í•­ëª©ì„ '{file_path}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return items
    except FileNotFoundError:
        print(f"ğŸš¨ ê²½ê³ : '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return []

APPROVED_TEMPLATES = load_line_by_line("./data/approved_templates.txt")
REJECTED_TEMPLATES_TEXT = load_by_separator("./data/rejected_templates.txt")

# ì „ì—­ ë³€ìˆ˜ë¡œ Retrieverë¥¼ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ë¡œë”© ë°©ì§€
retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected = None, None, None, None

def setup_retrievers():
    global retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected
    if all([retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected]):
        print("Retrieverê°€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print("Retriever ì„¤ì • ì‹œì‘...")
    
    from chromadb.config import Settings

    docs_compliance = TextLoader("./data/compliance_rules.txt", encoding='utf-8').load()
    docs_generation = TextLoader("./data/generation_rules.txt", encoding='utf-8').load()
    
    docs_whitelist = [Document(page_content=t) for t in APPROVED_TEMPLATES]
    docs_rejected = [Document(page_content=t) for t in REJECTED_TEMPLATES_TEXT]

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_compliance = text_splitter.split_documents(docs_compliance)
    split_generation = text_splitter.split_documents(docs_generation)
    split_rejected = docs_rejected

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_db_path = "./vector_db"
    
    client_settings = Settings(anonymized_telemetry=False)
    
    db_compliance = Chroma.from_documents(split_compliance, embeddings, collection_name="compliance_rules", persist_directory=vector_db_path, client_settings=client_settings)
    db_generation = Chroma.from_documents(split_generation, embeddings, collection_name="generation_rules", persist_directory=vector_db_path, client_settings=client_settings)
    db_whitelist = Chroma.from_documents(docs_whitelist, embeddings, collection_name="whitelist_templates", persist_directory=vector_db_path, client_settings=client_settings)
    db_rejected = Chroma.from_documents(split_rejected, embeddings, collection_name="rejected_templates", persist_directory=vector_db_path, client_settings=client_settings)

    def create_hybrid_retriever(vectorstore, docs):
        if not docs:
            return vectorstore.as_retriever(search_kwargs={"k": 5})

        vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        keyword_retriever = BM25Retriever.from_documents(docs)
        keyword_retriever.k = 5

        ensemble_retriever = EnsembleRetriever(retrievers=[vector_retriever, keyword_retriever], weights=[0.5, 0.5])
        
        if Ranker:
            compressor = FlashRankRerank(top_n=3)
            return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble_retriever)
        return ensemble_retriever

    retriever_compliance = create_hybrid_retriever(db_compliance, split_compliance)
    retriever_generation = create_hybrid_retriever(db_generation, split_generation)
    retriever_whitelist = create_hybrid_retriever(db_whitelist, docs_whitelist)
    retriever_rejected = create_hybrid_retriever(db_rejected, split_rejected)
    
    print("âœ… Retriever ì„¤ì • ì™„ë£Œ!")

# --- 2. LangGraph íŒŒì´í”„ë¼ì¸ ì •ì˜ ---
class GraphState(TypedDict):
    original_request: str
    request_type: str
    hypothetical_template: str
    template_draft: str
    retrieved_examples: str
    retrieved_rules: str
    retrieved_rejections: str
    validation_result: str
    correction_attempts: int

llm = ChatOpenAI(model="gpt-4o", temperature=0.4)

# --- [ìˆ˜ì •ëœ ë¶€ë¶„] routing_promptì˜ ì¤‘ê´„í˜¸ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ ---
routing_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ëŠ” ë¼ìš°í„°ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ìš”ì²­ì´ 'ì§§ì€ ì˜ë„'ì¸ì§€ 'ê¸´ ì´ˆì•ˆ'ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

    - 'ì§§ì€ ì˜ë„': í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ëœ, ì•„ì´ë””ì–´ë‚˜ ì£¼ì œì— ê°€ê¹Œìš´ ìš”ì²­. (ì˜ˆ: "ì£¼ë¬¸ ì™„ë£Œ ë©”ì‹œì§€ ë§Œë“¤ì–´ì¤˜")
    - 'ê¸´ ì´ˆì•ˆ': ì—¬ëŸ¬ ì¤„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, #{{ë³€ìˆ˜}} ë“±ì„ í¬í•¨í•˜ì—¬ ì´ë¯¸ ì™„ì„±ëœ í…œí”Œë¦¿ í˜•íƒœì— ê°€ê¹Œìš´ ìš”ì²­.

    ì˜¤ì§ 'intent' ë˜ëŠ” 'draft' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

    ì‚¬ìš©ì ìš”ì²­:
    {request}
    """
)

hyde_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê³  ê·¸ì— ë§ëŠ” ì´ìƒì ì¸ ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ìƒìƒí•´ë‚´ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì˜¤ì§ ìš”ì²­ì— ë¶€í•©í•˜ëŠ” ì™„ë²½í•œ í…œí”Œë¦¿ í…ìŠ¤íŠ¸ë§Œ ìƒì„±í•˜ê³ , ë‹¤ë¥¸ ì–´ë–¤ ì„¤ëª…ë„ ë§ë¶™ì´ì§€ ë§ˆì„¸ìš”.

    ì‚¬ìš©ì ìš”ì²­: "{request}"

    ê°€ìƒ í…œí”Œë¦¿:
    """
)

generation_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ì‹¤ìˆ˜ë¥¼ í†µí•´ ë°°ìš°ëŠ” ì•Œë¦¼í†¡ í…œí”Œë¦¿ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    # ì§€ì‹œì‚¬í•­
    1. 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ì •ë³´ì„± ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ì‘ì„±í•˜ì„¸ìš”.
    2. **(ì¤‘ìš”) 'ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€'ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ì—¬ ë™ì¼í•œ ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.**
    3. 'ì°¸ê³ ìš© ì„±ê³µ ì‚¬ë¡€'ì˜ ìŠ¤íƒ€ì¼ì„ ë”°ë¥´ë˜, ì ˆëŒ€ ë˜‘ê°™ì´ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
    4. 'í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™'ì„ ì§€í‚¤ê³ , #{{ë³€ìˆ˜}} ì‚¬ìš©ë²•ì„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
    5. ë§Œì•½ ì‚¬ìš©ì ìš”ì²­ì´ ê´‘ê³ ì„±ì´ë¼ë©´, ì •ë³´ì„±ìœ¼ë¡œ ë°œì†¡ ë¶ˆê°€í•˜ë‹¤ê³  ì•ˆë‚´í•˜ê³  'ì¹œêµ¬í†¡' ì‚¬ìš©ì„ ê¶Œì¥í•˜ì„¸ìš”.

    # ì‚¬ìš©ì ì›ë³¸ ìš”ì²­
    {original_request}

    # ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€ (ì˜¤ë‹µ ë…¸íŠ¸)
    {rejections}

    # ì°¸ê³ ìš© ì„±ê³µ ì‚¬ë¡€
    {examples}

    # í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™
    {rules}

    # ì „ë¬¸ê°€ ë‹µë³€ (ìœ„ ì§€ì‹œì‚¬í•­ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ì‘ì„±):
    """
)

step_back_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ë¬¸ì œì˜ ë³¸ì§ˆì„ íŒŒì•…í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ 'í…œí”Œë¦¿ ì´ˆì•ˆ'ì„ ë³´ê³ , ì´ í…œí”Œë¦¿ì´ ì •ë³´í†µì‹ ë§ë²•ì´ë‚˜ ë‚´ë¶€ ê·œì •ê³¼ ê´€ë ¨í•˜ì—¬ ì–´ë–¤ í•µì‹¬ì ì¸ ì§ˆë¬¸ì„ ë˜ì§€ëŠ”ì§€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.

    # í…œí”Œë¦¿ ì´ˆì•ˆ
    {draft}

    # í•µì‹¬ ì§ˆë¬¸:
    """
)

validation_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ê³¼ê±° íŒë¡€ê¹Œì§€ ì°¸ê³ í•˜ëŠ” ë§¤ìš° ê¼¼ê¼¼í•œ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
    
    # ê²€ìˆ˜ ëŒ€ìƒ í…œí”Œë¦¿
    {draft}

    # ê´€ë ¨ ê·œì •
    {rules}

    # ìœ ì‚¬í•œ ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€ (íŒë¡€)
    {rejections}

    # ì§€ì‹œì‚¬í•­
    1. 'ê²€ìˆ˜ ëŒ€ìƒ í…œí”Œë¦¿'ì´ 'ìœ ì‚¬í•œ ê³¼ê±° ë°˜ë ¤ ì‚¬ë¡€'ì™€ ë¹„ìŠ·í•œ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.
    2. ê·¸ ë‹¤ìŒ 'ê´€ë ¨ ê·œì •'ì„ ì¢…í•©ì ìœ¼ë¡œ ê²€í† í•˜ì—¬ ìµœì¢… ìœ„ë°˜ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
    3. ìœ„ë°˜ ì‚¬í•­ì´ ì—†ë‹¤ë©´ "accept" ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
    4. ìœ„ë°˜ ì‚¬í•­ì´ ìˆë‹¤ë©´, ì–´ë–¤ ê·œì • ë˜ëŠ” ê³¼ê±° ì‚¬ë¡€ë¥¼ ê·¼ê±°ë¡œ ìœ„ë°˜ì¸ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """
)

def route_request(state: GraphState):
    print("\n--- 1. ìš”ì²­ ìœ í˜• ë¶„ì„ (ë¼ìš°í„°) ---")
    request = state['original_request']
    
    chain = routing_prompt | llm | StrOutputParser()
    request_type = chain.invoke({"request": request})
    
    if "draft" in request_type.lower():
        state['request_type'] = "draft"
        print("âœ… ìš”ì²­ ìœ í˜•: ê¸´ ì´ˆì•ˆ (ê²€ì¦/ìˆ˜ì • ê²½ë¡œë¡œ ì§„í–‰)")
        state['template_draft'] = request
    else:
        state['request_type'] = "intent"
        print("âœ… ìš”ì²­ ìœ í˜•: ì§§ì€ ì˜ë„ (ì°½ì‘ ê²½ë¡œë¡œ ì§„í–‰)")
    
    return state

def hyde_generation(state: GraphState):
    print("\n--- 2a. HyDE ê°€ìƒ í…œí”Œë¦¿ ìƒì„± ì‹œì‘ (ì°½ì‘ ê²½ë¡œ) ---")
    request = state['original_request']
    chain = hyde_prompt | llm | StrOutputParser()
    hypothetical_template = chain.invoke({"request": request})
    state['hypothetical_template'] = hypothetical_template
    print("âœ… HyDE ìƒì„± ì™„ë£Œ.")
    return state

def generate_draft(state: GraphState):
    print("\n--- 2b. í…œí”Œë¦¿ ì´ˆì•ˆ ìƒì„± ì‹œì‘ (ì°½ì‘ ê²½ë¡œ) ---")
    original_request = state['original_request']
    query = state['hypothetical_template']
    
    example_docs = retriever_whitelist.invoke(query)
    state['retrieved_examples'] = "\n\n".join([f"ì˜ˆì‹œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(example_docs)])
    
    rule_docs = retriever_generation.invoke(query)
    state['retrieved_rules'] = "\n\n".join([doc.page_content for doc in rule_docs])
    
    rejected_docs = retriever_rejected.invoke(query)
    state['retrieved_rejections'] = "\n\n".join([doc.page_content for doc in rejected_docs])
    
    chain = generation_prompt | llm | StrOutputParser()
    draft = chain.invoke({
        "original_request": original_request, 
        "rejections": state['retrieved_rejections'],
        "examples": state['retrieved_examples'], 
        "rules": state['retrieved_rules']
    })
    state['template_draft'] = draft
    state['correction_attempts'] = 0
    print("âœ… ì´ˆì•ˆ ìƒì„± ì™„ë£Œ.")
    print(f"ìƒì„±ëœ ì´ˆì•ˆ:\n---\n{draft}\n---")
    return state

def validate_draft(state: GraphState):
    print("\n--- 3. ê·œì • ì¤€ìˆ˜ ê²€ì¦ ì‹œì‘ ---")
    draft = state['template_draft']
    
    if "ê´‘ê³ ì„±" in draft or "ì¹œêµ¬í†¡" in draft:
        state['validation_result'] = "accept"
        print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼ (ìƒì„± ë‹¨ê³„ì—ì„œ ê´‘ê³ ì„±ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ì•ˆë‚´í•¨)")
        return state

    print("   - 3a. Step-Back í•µì‹¬ ìŸì  ìƒì„± ì¤‘...")
    step_back_chain = step_back_prompt | llm | StrOutputParser()
    step_back_question = step_back_chain.invoke({"draft": draft})
    print(f"   - ìƒì„±ëœ í•µì‹¬ ìŸì : {step_back_question}")

    print("   - 3b. ê´€ë ¨ ê·œì • ë° ë°˜ë ¤ ì‚¬ë¡€ ê²€ìƒ‰ ì¤‘...")
    compliance_docs = retriever_compliance.invoke(step_back_question)
    rules = "\n\n".join([doc.page_content for doc in compliance_docs])
    
    rejected_docs = retriever_rejected.invoke(draft)
    rejections = "\n\n".join([doc.page_content for doc in rejected_docs])
    
    print("   - 3c. ìµœì¢… ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
    chain = validation_prompt | llm | StrOutputParser()
    result = chain.invoke({"draft": draft, "rules": rules, "rejections": rejections})
    state['validation_result'] = result
    
    if "accept" in result.lower():
        print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
    else:
        print(f"ğŸš¨ ê²€ì¦ ê²°ê³¼: ìœ„ë°˜ ë°œê²¬ (ì‹œë„ {state['correction_attempts'] + 1}íšŒ)")
        print(f"ìœ„ë°˜ ë‚´ìš©: {result}")
    return state

def self_correct(state: GraphState):
    print("\n--- 4. ìê°€ ìˆ˜ì • ì‹œì‘ ---")
    
    correction_request = (
        f"ì´ì „ í…œí”Œë¦¿ì€ ë‹¤ìŒì˜ ì´ìœ ë¡œ ë°˜ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤: '{state['validation_result']}'. "
        f"ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì›ë˜ ìš”ì²­ì¸ '{state['original_request']}'ì„ ë§Œì¡±í•˜ëŠ” ìƒˆë¡œìš´ ì •ë³´ì„± í…œí”Œë¦¿ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
    )
    
    chain = generation_prompt | llm | StrOutputParser()
    new_draft = chain.invoke({
        "original_request": correction_request,
        "rejections": state['retrieved_rejections'],
        "examples": state['retrieved_examples'],
        "rules": state['retrieved_rules']
    })
    
    state['template_draft'] = new_draft
    state['correction_attempts'] += 1
    print("âœ… ìê°€ ìˆ˜ì • ì™„ë£Œ.")
    return state

def decide_creation_path(state: GraphState):
    if state['request_type'] == "draft":
        return "validate_draft"
    else:
        return "hyde_generation"

def decide_next_step(state: GraphState):
    if "accept" in state['validation_result'].lower():
        return "end"
    elif state['correction_attempts'] >= 1:
        return "end_with_failure"
    else:
        return "self_correct"

def build_graph():
    workflow = StateGraph(GraphState)
    
    workflow.add_node("router", route_request)
    workflow.add_node("hyde_generation", hyde_generation)
    workflow.add_node("generate_draft", generate_draft)
    workflow.add_node("validate_draft", validate_draft)
    workflow.add_node("self_correct", self_correct)
    
    workflow.set_entry_point("router")
    
    workflow.add_conditional_edges(
        "router",
        decide_creation_path,
        {
            "hyde_generation": "hyde_generation",
            "validate_draft": "validate_draft"
        }
    )
    
    workflow.add_edge("hyde_generation", "generate_draft")
    workflow.add_edge("generate_draft", "validate_draft")
    
    workflow.add_conditional_edges(
        "validate_draft",
        decide_next_step,
        {"self_correct": "self_correct", "end": END, "end_with_failure": END}
    )
    workflow.add_edge("self_correct", "validate_draft")
    
    return workflow.compile()

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    setup_retrievers()
    app = build_graph()

    requests_to_test = {
        "ì§§ì€ ì˜ë„ (ì°½ì‘ ê²½ë¡œ í…ŒìŠ¤íŠ¸)": "ì•ˆë…•í•˜ì„¸ìš” #{íšŒì›ëª…}ë‹˜, ê³ ìš©ë…¸ë™ë¶€ í”Œë«í¼ ì´ìš© ì§€ì›ì— ëŒ€í•´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.",
        "ê¸´ ì´ˆì•ˆ (ê²€ì¦ ê²½ë¡œ í…ŒìŠ¤íŠ¸)": """ì•ˆë…•í•˜ì„¸ìš” #{íšŒì›ëª…}ë‹˜, ê³ ìš©ë…¸ë™ë¶€ í”Œë«í¼ ì´ìš© ì§€ì›ì— ëŒ€í•œ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.

        â–¶ ì§€ì› ëŒ€ìƒ: #{ì§€ì›ëŒ€ìƒ}
        â–¶ ì§€ì› ë‚´ìš©: #{ì§€ì›ë‚´ìš©}
        â–¶ ì‹ ì²­ ê¸°ê°„: #{ì‹ ì²­ê¸°ê°„}

        ìì„¸í•œ ë‚´ìš©ì€ ê³ ìš©ë…¸ë™ë¶€ í™ˆí˜ì´ì§€ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì—°ë½ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤."""
    }
    
    for test_name, user_request in requests_to_test.items():
        print(f"\n==================================================")
        print(f"í…ŒìŠ¤íŠ¸ ì‹œì‘: {test_name}")
        print(f"==================================================")
        print(f"ì‚¬ìš©ì ìš”ì²­:\n---\n{user_request}\n---")
        
        final_state = app.invoke({"original_request": user_request})

        print(f"\n================ ìµœì¢… ê²°ê³¼: {test_name} ================")
        if final_state and "accept" in final_state.get('validation_result', '').lower():
            print("ğŸ‰ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
            print(final_state['template_draft'])
        elif final_state:
            print("ğŸ”¥ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì‹œë„ ê²°ê³¼:")
            print(f"ì‹¤íŒ¨ ì‚¬ìœ : {final_state.get('validation_result', 'N/A')}")
            print(f"ë§ˆì§€ë§‰ ì´ˆì•ˆ:\n{final_state.get('template_draft', 'N/A')}")
        else:
            print("ì˜¤ë¥˜: íŒŒì´í”„ë¼ì¸ì´ ìµœì¢… ìƒíƒœë¥¼ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("============================================")
