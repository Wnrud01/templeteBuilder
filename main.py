import os
import chromadb
from typing import TypedDict, List

# Pydantic ë° LangChain í˜¸í™˜ì„±ì„ ìœ„í•œ ì„í¬íŠ¸
from pydantic import PrivateAttr

# LangChain ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

# FlashRank ì„í¬íŠ¸
try:
    from flashrank import Ranker, RerankRequest
    from langchain.retrievers.document_compressors.base import BaseDocumentCompressor
    from langchain_core.callbacks.manager import Callbacks
except ImportError:
    print("FlashRank ë˜ëŠ” ê´€ë ¨ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pip install flashrank'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    BaseDocumentCompressor = object # ì„ì‹œ ì •ì˜
    Ranker = None

# ValidationError í•´ê²°ì„ ìœ„í•´ PrivateAttr ì‚¬ìš©
class FlashRankRerank(BaseDocumentCompressor):
    """FlashRankë¥¼ ì‚¬ìš©í•œ LangChain ë¬¸ì„œ ì¬ì •ë ¬ê¸°"""
    
    _ranker: Ranker = PrivateAttr()
    top_n: int = 3

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._ranker = Ranker()

    class Config:
        arbitrary_types_allowed = True

    def compress_documents(
        self,
        documents: List[Document],
        query: str,
        callbacks: Callbacks | None = None,
    ) -> List[Document]:
        if not documents:
            return []
        
        rerank_request = RerankRequest(
            query=query,
            passages=[{"id": i, "text": doc.page_content} for i, doc in enumerate(documents)]
        )
        reranked_results = self._ranker.rerank(rerank_request)
        
        final_docs = []
        for item in reranked_results[:self.top_n]:
            original_doc_index = item['id']
            doc = documents[original_doc_index]
            doc.metadata["relevance_score"] = item['score']
            final_docs.append(doc)
        return final_docs

# --- 0. ì´ˆê¸° ì„¤ì •: API í‚¤ ë¡œë”© ---
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")

# --- 1. ë°ì´í„° ì¤€ë¹„: ì™¸ë¶€ íŒŒì¼ì—ì„œ í…œí”Œë¦¿ ë¡œë”© ---
def load_templates_from_file(file_path: str) -> List[str]:
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            templates = [line.strip() for line in f if line.strip()]
        print(f"âœ… {len(templates)}ê°œì˜ í…œí”Œë¦¿ì„ '{file_path}'ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return templates
    except FileNotFoundError:
        print(f"ğŸš¨ ê²½ê³ : '{file_path}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return []

APPROVED_TEMPLATES = load_templates_from_file("./data/approved_templates.txt")

retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected = None, None, None, None

def setup_retrievers():
    global retriever_compliance, retriever_generation, retriever_whitelist, retriever_rejected
    if retriever_compliance and retriever_generation and retriever_whitelist and retriever_rejected:
        print("Retrieverê°€ ì´ë¯¸ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return

    print("Retriever ì„¤ì • ì‹œì‘...")
    
    from chromadb.config import Settings

    docs_compliance = TextLoader("./data/compliance_rules.txt", encoding='utf-8').load()
    docs_generation = TextLoader("./data/generation_rules.txt", encoding='utf-8').load()
    docs_whitelist = [Document(page_content=t) for t in APPROVED_TEMPLATES]
    docs_rejected = TextLoader("./data/rejected_templates.txt", encoding='utf-8').load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_compliance = text_splitter.split_documents(docs_compliance)
    split_generation = text_splitter.split_documents(docs_generation)
    split_rejected = text_splitter.split_documents(docs_rejected)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    vector_db_path = "./vector_db"
    
    client_settings = Settings(anonymized_telemetry=False)
    
    db_compliance = Chroma.from_documents(split_compliance, embeddings, collection_name="compliance_rules", persist_directory=vector_db_path, client_settings=client_settings)
    db_generation = Chroma.from_documents(split_generation, embeddings, collection_name="generation_rules", persist_directory=vector_db_path, client_settings=client_settings)
    db_whitelist = Chroma.from_documents(docs_whitelist, embeddings, collection_name="whitelist_templates", persist_directory=vector_db_path, client_settings=client_settings)
    db_rejected = Chroma.from_documents(split_rejected, embeddings, collection_name="rejected_templates", persist_directory=vector_db_path, client_settings=client_settings)

    def create_hybrid_retriever(vectorstore, docs):
        if not docs:
            return vectorstore.as_retriever(search_kwargs={"k": 7})

        vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 7})
        keyword_retriever = BM25Retriever.from_documents(docs)
        keyword_retriever.k = 7

        ensemble_retriever = EnsembleRetriever(retrievers=[vector_retriever, keyword_retriever], weights=[0.5, 0.5])
        
        if Ranker:
            compressor = FlashRankRerank(top_n=3)
            return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=ensemble_retriever)
        return ensemble_retriever

    retriever_compliance = create_hybrid_retriever(db_compliance, split_compliance)
    retriever_generation = create_hybrid_retriever(db_generation, split_generation)
    retriever_whitelist = create_hybrid_retriever(db_whitelist, docs_whitelist)
    retriever_rejected = create_hybrid_retriever(db_rejected, split_rejected)
    
    print("âœ… Retriever ì„¤ì • ì™„ë£Œ!")

# --- 2. LangGraph íŒŒì´í”„ë¼ì¸ ì •ì˜ ---
class GraphState(TypedDict):
    original_request: str
    expanded_queries: List[str] # [ìˆ˜ì •] HyDE í•„ë“œë¥¼ ì¿¼ë¦¬ í™•ì¥ í•„ë“œë¡œ ë³€ê²½
    template_draft: str
    retrieved_examples: str
    retrieved_rules: str
    retrieved_rejected_examples: str
    validation_result: str
    correction_attempts: int

llm = ChatOpenAI(model="gpt-4o", temperature=0.4)

# --- [ì¶”ê°€] ì¿¼ë¦¬ í™•ì¥ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ---
expansion_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬, ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë° ê°€ì¥ íš¨ê³¼ì ì¸ ì—¬ëŸ¬ ê°œì˜ ê²€ìƒ‰ ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­ì„ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ìµœì†Œ 3ê°œì˜ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
    ê° ì§ˆë¬¸ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

    # ì›ë³¸ ìš”ì²­:
    {original_request}

    # í™•ì¥ëœ ê²€ìƒ‰ ì§ˆë¬¸:
    """
)

generation_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì°½ì˜ì ì´ê³  ê·œì •ì„ ì˜ ì•„ëŠ” ì•Œë¦¼í†¡ í…œí”Œë¦¿ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì •ë³´ì„± ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

    # ì§€ì‹œì‚¬í•­
    1. ë¨¼ì € 'ì‚¬ìš©ì ì›ë³¸ ìš”ì²­'ì´ ì •ë³´ì„±ì¸ì§€ ê´‘ê³ ì„±ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
    2. **[ì •ë³´ì„± ìš”ì²­ì˜ ê²½ìš°]**
        - ìš”ì²­ì„ ë§Œì¡±í•˜ëŠ” ìƒˆë¡œìš´ ì •ë³´ì„± ì•Œë¦¼í†¡ í…œí”Œë¦¿ì„ ì‘ì„±í•˜ì„¸ìš”.
        - 'ì°¸ê³ ìš© ìŠ¹ì¸ í…œí”Œë¦¿'ê³¼ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ì„ ë”°ë¥´ë˜, ì ˆëŒ€ ë˜‘ê°™ì´ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
        - 'ì°¸ê³ ìš© ì‹¤íŒ¨ ì‚¬ë¡€'ë¥¼ ë³´ê³  ë™ì¼í•œ ì‹¤ìˆ˜ë¥¼ ì ˆëŒ€ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.
        - 'í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™'ì„ ë°˜ë“œì‹œ ì§€í‚¤ê³ , #{{ë³€ìˆ˜}} ì‚¬ìš©ë²•ì„ ì ì ˆíˆ í™œìš©í•˜ì„¸ìš”.
    3. **[ê´‘ê³ ì„± ìš”ì²­ì˜ ê²½ìš°]**
        - í•´ë‹¹ ìš”ì²­ì€ ê´‘ê³ ì„± ë‚´ìš©(ì˜ˆ: í• ì¸, ì¿ í°, ì´ë²¤íŠ¸)ì„ í¬í•¨í•˜ì—¬ ì •ë³´ì„± ì•Œë¦¼í†¡ìœ¼ë¡œ ë°œì†¡ì´ ë¶ˆê°€í•˜ë‹¤ê³  ëª…í™•íˆ ì„¤ëª…í•˜ì„¸ìš”.
        - ì‚¬ìš©ìê°€ 'ì¹œêµ¬í†¡'ê³¼ ê°™ì€ ê´‘ê³ ì„± ë©”ì‹œì§€ ì±„ë„ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ê³  ì•ˆë‚´í•˜ì„¸ìš”.

    # ì‚¬ìš©ì ì›ë³¸ ìš”ì²­
    {original_request}

    # ì°¸ê³ ìš© ìŠ¹ì¸ í…œí”Œë¦¿ (ì •ë³´ì„± ìš”ì²­ì¼ ê²½ìš°ì—ë§Œ ì°¸ê³ )
    {examples}

    # ì°¸ê³ ìš© ì‹¤íŒ¨ ì‚¬ë¡€ (ì´ëŸ° ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”)
    {rejected_examples}

    # í•„ìˆ˜ ì¤€ìˆ˜ ê·œì¹™ (ì •ë³´ì„± í…œí”Œë¦¿ ìƒì„± ì‹œ ì°¸ê³ )
    {rules}

    # ì „ë¬¸ê°€ ë‹µë³€ (ìœ„ ì§€ì‹œì‚¬í•­ì„ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ì‘ì„±):
    """
)

validation_prompt = ChatPromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ì •ë³´í†µì‹ ë§ë²• ë° ë‚´ë¶€ ê·œì •ì„ ê²€ìˆ˜í•˜ëŠ” ë§¤ìš° ê¼¼ê¼¼í•œ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.
    
    # ê²€ìˆ˜ ëŒ€ìƒ í…œí”Œë¦¿
    {draft}

    # ê´€ë ¨ ê·œì •
    {rules}

    # ìœ ì‚¬í•œ ë°˜ë ¤ ì‚¬ë¡€ (ì°¸ê³ ìš©)
    {rejected_examples}

    # ì§€ì‹œì‚¬í•­
    1. 'ê²€ìˆ˜ ëŒ€ìƒ í…œí”Œë¦¿'ì´ **ì •ë³´ì„±ì¸ì§€ ê´‘ê³ ì„±ì¸ì§€ íŒë‹¨**í•˜ì„¸ìš”.
    2. **[íŒë‹¨ ê²°ê³¼ê°€ ì •ë³´ì„±ì¼ ê²½ìš°]**
        - 'ê´€ë ¨ ê·œì •'ê³¼ 'ìœ ì‚¬í•œ ë°˜ë ¤ ì‚¬ë¡€'ë¥¼ ì°¸ê³ í•˜ì—¬ ìœ„ë°˜ ì‚¬í•­ì´ ì—†ëŠ”ì§€ ê²€í† í•˜ì„¸ìš”.
        - ìœ„ë°˜ ì‚¬í•­ì´ ì—†ë‹¤ë©´ "accept" ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        - ìœ„ë°˜ ì‚¬í•­ì´ ìˆë‹¤ë©´, êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    3. **[íŒë‹¨ ê²°ê³¼ê°€ ê´‘ê³ ì„±ì¼ ê²½ìš°]**
        - "ê´‘ê³ ì„± ë©”ì‹œì§€ë¡œ íŒë‹¨ë¨." ì´ë¼ê³  ë°íˆê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    """
)

# --- [ì¶”ê°€] ì¿¼ë¦¬ í™•ì¥ ë…¸ë“œ ---
def query_expansion(state: GraphState):
    print("\n--- 1. ì¿¼ë¦¬ í™•ì¥ ì‹œì‘ ---")
    original_request = state['original_request']
    
    chain = expansion_prompt | llm | StrOutputParser()
    result = chain.invoke({"original_request": original_request})
    
    queries = [q.strip() for q in result.strip().split("\n") if q.strip()]
    state['expanded_queries'] = queries
    
    print("âœ… ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ.")
    print(f"í™•ì¥ëœ ì¿¼ë¦¬:\n---\n{queries}\n---")
    return state

# --- [ìˆ˜ì •] generate_draftê°€ ë‹¤ì¤‘ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì • ---
def generate_draft(state: GraphState):
    print("\n--- 2. ë‹¤ì¤‘ ì¿¼ë¦¬ ê¸°ë°˜ ì •ë³´ ê²€ìƒ‰ ë° ì´ˆì•ˆ ìƒì„± ---")
    original_request = state['original_request']
    queries = state['expanded_queries']
    
    # ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹¨
    all_example_docs, all_rule_docs, all_rejected_docs = [], [], []
    for q in queries:
        all_example_docs.extend(retriever_whitelist.invoke(q))
        all_rule_docs.extend(retriever_generation.invoke(q))
        all_rejected_docs.extend(retriever_rejected.invoke(q))

    # ì¤‘ë³µëœ ë¬¸ì„œ ì œê±° (í˜ì´ì§€ ë‚´ìš© ê¸°ì¤€)
    unique_examples = list({doc.page_content: doc for doc in all_example_docs}.values())
    unique_rules = list({doc.page_content: doc for doc in all_rule_docs}.values())
    unique_rejected = list({doc.page_content: doc for doc in all_rejected_docs}.values())

    # í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•  í˜•íƒœë¡œ ë³€í™˜
    examples = "\n\n".join([f"ì˜ˆì‹œ {i+1}:\n{doc.page_content}" for i, doc in enumerate(unique_examples)])
    rules = "\n\n".join([doc.page_content for doc in unique_rules])
    rejected_examples = "\n\n".join([f"ì‹¤íŒ¨ ì‚¬ë¡€ {i+1}:\n{doc.page_content}" for i, doc in enumerate(unique_rejected)])

    # ì´í›„ ë‹¨ê³„ì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ìƒíƒœì— ì €ì¥
    state['retrieved_examples'] = examples
    state['retrieved_rules'] = rules
    state['retrieved_rejected_examples'] = rejected_examples
    
    chain = generation_prompt | llm | StrOutputParser()
    draft = chain.invoke({
        "original_request": original_request, 
        "examples": examples, 
        "rejected_examples": rejected_examples,
        "rules": rules
    })
    state['template_draft'] = draft
    state['correction_attempts'] = 0
    print("âœ… ì´ˆì•ˆ ìƒì„± ì™„ë£Œ.")
    print(f"ìƒì„±ëœ ì´ˆì•ˆ:\n---\n{draft}\n---")
    return state

def validate_draft(state: GraphState):
    print("\n--- 3. ê·œì • ì¤€ìˆ˜ ê²€ì¦ ì‹œì‘ ---")
    draft = state['template_draft']
    
    if "ê´‘ê³ ì„±" in draft or "ì¹œêµ¬í†¡" in draft:
        state['validation_result'] = "accept"
        print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼ (ìƒì„± ë‹¨ê³„ì—ì„œ ê´‘ê³ ì„±ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ì•ˆë‚´í•¨)")
        return state

    compliance_docs = retriever_compliance.invoke(draft)
    rules = "\n\n".join([doc.page_content for doc in compliance_docs])

    rejected_docs = retriever_rejected.invoke(draft)
    rejected_examples = "\n\n".join([f"ìœ ì‚¬ ì‹¤íŒ¨ ì‚¬ë¡€ {i+1}:\n{doc.page_content}" for i, doc in enumerate(rejected_docs)])
    
    chain = validation_prompt | llm | StrOutputParser()
    result = chain.invoke({
        "draft": draft, 
        "rules": rules,
        "rejected_examples": rejected_examples
    })
    state['validation_result'] = result
    
    if "accept" in result.lower():
        print("âœ… ê²€ì¦ ê²°ê³¼: í†µê³¼")
    else:
        print(f"ğŸš¨ ê²€ì¦ ê²°ê³¼: ìœ„ë°˜ ë°œê²¬ (ì‹œë„ {state['correction_attempts'] + 1}íšŒ)")
        print(f"ìœ„ë°˜ ë‚´ìš©: {result}")
    return state

def self_correct(state: GraphState):
    print("\n--- 4. ìê°€ ìˆ˜ì • ì‹œì‘ ---")
    
    correction_request = (
        f"ì´ì „ í…œí”Œë¦¿ì€ ë‹¤ìŒì˜ ì´ìœ ë¡œ ë°˜ë ¤ë˜ì—ˆìŠµë‹ˆë‹¤: '{state['validation_result']}'. "
        f"ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ì›ë˜ ìš”ì²­ì¸ '{state['original_request']}'ì„ ë§Œì¡±í•˜ëŠ” ìƒˆë¡œìš´ ì •ë³´ì„± í…œí”Œë¦¿ì„ ìƒì„±í•´ì£¼ì„¸ìš”."
    )
    
    chain = generation_prompt | llm | StrOutputParser()
    new_draft = chain.invoke({
        "original_request": correction_request,
        "examples": state['retrieved_examples'],
        "rejected_examples": state['retrieved_rejected_examples'],
        "rules": state['retrieved_rules']
    })
    
    state['template_draft'] = new_draft
    state['correction_attempts'] += 1
    print("âœ… ìê°€ ìˆ˜ì • ì™„ë£Œ.")
    print(f"ìˆ˜ì •ëœ ì´ˆì•ˆ:\n---\n{new_draft}\n---")
    return state

def decide_next_step(state: GraphState):
    if "accept" in state['validation_result'].lower():
        return "end"
    elif state['correction_attempts'] >= 1:
        return "end_with_failure"
    else:
        return "self_correct"

# --- [ìˆ˜ì •] ê·¸ë˜í”„ ë¹Œë“œ ë¡œì§ ìˆ˜ì • ---
def build_graph():
    workflow = StateGraph(GraphState)
    
    workflow.add_node("query_expansion", query_expansion)
    workflow.add_node("generate_draft", generate_draft)
    workflow.add_node("validate_draft", validate_draft)
    workflow.add_node("self_correct", self_correct)
    
    workflow.set_entry_point("query_expansion")
    
    workflow.add_edge("query_expansion", "generate_draft")
    workflow.add_edge("generate_draft", "validate_draft")
    workflow.add_conditional_edges(
        "validate_draft",
        decide_next_step,
        {"self_correct": "self_correct", "end": END, "end_with_failure": END}
    )
    workflow.add_edge("self_correct", "validate_draft")
    
    return workflow.compile()

# --- 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
if __name__ == "__main__":
    setup_retrievers()
    app = build_graph()

    # í…ŒìŠ¤íŠ¸í•  ì‚¬ìš©ì ìš”ì²­
    user_request = "ì•ˆë…•í•˜ì„¸ìš” #{íšŒì›ëª…}ë‹˜, ê³ ìš©ë…¸ë™ë¶€ í”Œë«í¼ ì´ìš© ì§€ì›ì— ëŒ€í•´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤. ê³ ê°ì„¼í„° ë²ˆí˜¸ëŠ” 02-1111-1111ì…ë‹ˆë‹¤."
    
    print(f"\n==================================================")
    print(f"ì‚¬ìš©ì ìš”ì²­: {user_request}")
    print(f"==================================================")
    
    final_state = app.invoke({"original_request": user_request})

    print(f"\n================ ìµœì¢… ê²°ê³¼ ================")
    if final_state and "accept" in final_state.get('validation_result', '').lower():
        print("ğŸ‰ ìµœì¢… ìƒì„±ëœ ë‹µë³€:")
        print(final_state['template_draft'])
    elif final_state:
        print("ğŸ”¥ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì‹œë„ ê²°ê³¼:")
        print(f"ì‹¤íŒ¨ ì‚¬ìœ : {final_state.get('validation_result', 'N/A')}")
        print(f"ë§ˆì§€ë§‰ ì´ˆì•ˆ:\n{final_state.get('template_draft', 'N/A')}")
    else:
        print("ì˜¤ë¥˜: íŒŒì´í”„ë¼ì¸ì´ ìµœì¢… ìƒíƒœë¥¼ ë°˜í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    print("============================================")